# app.py
import os
import pandas as pd
import numpy as np
import re
import joblib
import json # –î–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ 'body'
import logging
from flask import Flask, request, jsonify
from datetime import datetime
# from sklearn.preprocessing import LabelEncoder # –ù–µ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º, —Ç–∞–∫ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
# –£—Ä–æ–≤–µ–Ω—å INFO –¥–ª—è Railway, —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å –≤–∞–∂–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –ª–æ–≥–∞—Ö
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Flask –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ---
app = Flask(__name__)

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏ –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ ---
model = None
scaler = None
encoders = None # –í –Ω–æ—É—Ç–±—É–∫–µ —ç—Ç–æ label_encoders
features = None # –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
model_name = None
ensemble_weights = None

# --- –§—É–Ω–∫—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö (—Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω—ã –∏–∑ –≤–∞—à–µ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞) ---

def parse_string_number(value):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö —á–∏—Å–µ–ª"""
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–∞—Å—Å–∏–≤–æ–≤/—Å–µ—Ä–∏–π –≤ —Å–∫–∞–ª—è—Ä –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
    if isinstance(value, (list, np.ndarray, pd.Series)):
        if len(value) == 0:
            value = '' 
        else:
            value = value[0] if not isinstance(value, pd.Series) else (value.iloc[0] if len(value) > 0 else '')
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN, None, –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    if pd.isna(value) or value == '' or value == 'N/A' or value is None:
        return 0.0
    # –ï—Å–ª–∏ —É–∂–µ —á–∏—Å–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ
    if isinstance(value, (int, float)):
        return float(value)
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É –∏ –æ—á–∏—â–∞–µ–º
    value = str(value).upper().replace(',', '').strip()
    value = re.sub(r'[^\d\.\-KMBkmb]', '', value) # –£–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª—ã –≤–∞–ª—é—Ç –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
    if value == '' or value == '-':
        return 0.0
    try:
        if 'K' in value or 'k' in value:
            return float(value.replace('K', '').replace('k', '')) * 1_000
        elif 'M' in value or 'm' in value:
            return float(value.replace('M', '').replace('m', '')) * 1_000_000
        elif 'B' in value or 'b' in value:
            return float(value.replace('B', '').replace('b', '')) * 1_000_000_000
        else:
            return float(value)
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏ '{value}': {e}")
        return 0.0

def parse_time_to_minutes(value):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä–µ–º–µ–Ω–∏"""
     # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–∞—Å—Å–∏–≤–æ–≤/—Å–µ—Ä–∏–π –≤ —Å–∫–∞–ª—è—Ä –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
    if isinstance(value, (list, np.ndarray, pd.Series)):
        if len(value) == 0:
            value = '' 
        else:
            value = value[0] if not isinstance(value, pd.Series) else (value.iloc[0] if len(value) > 0 else '')
    if pd.isna(value) or value == '' or value == 'N/A' or value is None:
        return 0.0
    if isinstance(value, (int, float)):
        return float(value)
    total_minutes = 0.0
    value = str(value).lower().strip()
    try:
        # –î–Ω–∏
        days = re.findall(r'(\d+(?:\.\d+)?)d', value)
        if days:
            total_minutes += float(days[0]) * 1440
        # –ß–∞—Å—ã
        hours = re.findall(r'(\d+(?:\.\d+)?)h', value)
        if hours:
            total_minutes += float(hours[0]) * 60
        # –ú–∏–Ω—É—Ç—ã
        minutes = re.findall(r'(\d+(?:\.\d+)?)m(?!s)', value)
        if minutes:
            total_minutes += float(minutes[0])
        # –°–µ–∫—É–Ω–¥—ã
        seconds = re.findall(r'(\d+(?:\.\d+)?)s', value)
        if seconds:
            total_minutes += float(seconds[0]) / 60
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–±—É–µ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ —á–∏—Å–ª–æ –º–∏–Ω—É—Ç
        if total_minutes == 0:
            clean_value = re.sub(r'[^\d\.]', '', value)
            if clean_value:
                total_minutes = float(clean_value)
        return total_minutes
    except Exception as e:
         logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä–µ–º–µ–Ω–∏ '{value}': {e}")
         return 0.0

def parse_top10_holdings(value, total_top10_percent=None):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ –∫–∏—Ç–æ–≤ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    # --- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ value –≤ —Å–∫–∞–ª—è—Ä ---
    if isinstance(value, (list, np.ndarray, pd.Series)):
        if len(value) == 0:
             value_scalar = '' 
        else:
             value_scalar = value[0] if not isinstance(value, pd.Series) else (value.iloc[0] if len(value) > 0 else '')
    elif pd.isna(value) or value is None: 
         value_scalar = '' 
    else:
         value_scalar = str(value) 

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
    if value_scalar == '' or value_scalar == 'N/A' or value_scalar.lower() == 'nan':
        return {
            'top1_real_percent': 0.0, 'top3_real_percent': 0.0, 'top5_real_percent': 0.0,
            'concentration_ratio': 0.0, 'internal_distribution': [0.0]*10,
            'gini_coefficient': 0.0, 'herfindahl_index': 0.0
        }
    # --- –ö–æ–Ω–µ—Ü –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è ---
    
    try:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏
        value_clean = value_scalar.strip('[]').replace(' ', '') 
        if value_clean:
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –∏ 'nan' –∑–Ω–∞—á–µ–Ω–∏—è
            internal_percentages = [float(x) for x in value_clean.split(',') if x.strip() and x.strip().lower() != 'nan']
        else:
             internal_percentages = []
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω—É–ª—è–º–∏
        while len(internal_percentages) < 10:
            internal_percentages.append(0.0)
            
        # –†–∞—Å—á–µ—Ç —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
        if total_top10_percent is None or pd.isna(total_top10_percent) or total_top10_percent <= 0:
            real_percentages = internal_percentages
        else:
            total_internal = sum(internal_percentages)
            if total_internal > 0:
                normalized_percentages = [x / total_internal for x in internal_percentages]
                real_percentages = [x * total_top10_percent / 100 for x in normalized_percentages]
            else:
                real_percentages = [0.0] * 10

        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        top1_real = real_percentages[0] if len(real_percentages) > 0 else 0.0
        top3_real = sum(real_percentages[:3]) if len(real_percentages) >= 3 else sum(real_percentages)
        top5_real = sum(real_percentages[:5]) if len(real_percentages) >= 5 else sum(real_percentages)

        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏
        total_internal_nonzero = sum([x for x in internal_percentages if x > 0])
        concentration_ratio = internal_percentages[0] / total_internal_nonzero if total_internal_nonzero > 0 else 0.0

        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –î–∂–∏–Ω–∏ (–Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è)
        sorted_percentages = sorted([x for x in internal_percentages if x > 0], reverse=True)
        n = len(sorted_percentages)
        if n > 1:
            cumsum = np.cumsum(sorted_percentages)
            total_sorted_sum = sum(sorted_percentages)
            if total_sorted_sum > 0:
                gini_coefficient = (n + 1 - 2 * sum((n + 1 - i) * x for i, x in enumerate(cumsum))) / (n * total_sorted_sum)
            else:
                gini_coefficient = 0.0
        else:
            gini_coefficient = 0.0

        # –ò–Ω–¥–µ–∫—Å –•–µ—Ä—Ñ–∏–Ω–¥–∞–ª—è (–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è —Ä—ã–Ω–∫–∞)
        total_sum = sum(internal_percentages)
        if total_sum > 0:
            herfindahl_index = sum((x / total_sum) ** 2 for x in internal_percentages if x > 0)
        else:
            herfindahl_index = 0.0

        return {
            'top1_real_percent': float(top1_real),
            'top3_real_percent': float(top3_real),
            'top5_real_percent': float(top5_real),
            'concentration_ratio': float(concentration_ratio),
            'internal_distribution': [float(x) for x in internal_percentages],
            'gini_coefficient': float(gini_coefficient),
            'herfindahl_index': float(herfindahl_index)
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ parse_top10_holdings –¥–ª—è –∑–Ω–∞—á–µ–Ω–∏—è '{value}' (scalar: '{value_scalar}'), total_top10_percent='{total_top10_percent}': {e}")
        return {
            'top1_real_percent': 0.0, 'top3_real_percent': 0.0, 'top5_real_percent': 0.0,
            'concentration_ratio': 0.0, 'internal_distribution': [0.0]*10,
            'gini_coefficient': 0.0, 'herfindahl_index': 0.0
        }

# --- –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∏–∑ —è—á–µ–π–∫–∏ 18 –Ω–æ—É—Ç–±—É–∫–∞) ---
def predict_memtoken_advanced(token_data):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —É—Å–ø–µ—Ö–∞ –º–µ–º—Ç–æ–∫–µ–Ω–∞
    Args:
        token_data (dict): –î–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∞
    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    """
    global model, scaler, encoders, features, model_name, ensemble_weights

    if model is None or scaler is None or encoders is None or features is None:
        logger.error("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
        return {
            "success": False,
            "error": "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ —Å–µ—Ä–≤–µ—Ä–∞.",
            "probability": 0.5,
            "prediction": 0,
            "recommendation": "‚ùì –û–®–ò–ë–ö–ê - –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞",
            "confidence_interval": [0.0, 1.0]
        }

    try:
        logger.debug(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞: {token_data}")
        # --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ DataFrame ---
        token_df = pd.DataFrame([token_data])
        
        # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–∫–∞–∫ –≤ –Ω–æ—É—Ç–±—É–∫–µ) ---
        string_cols = ['market_cap', 'liquidity', 'ath']
        for col in string_cols:
            if col in token_df.columns:
                logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–æ–ª–±—Ü–∞ '{col}'")
                token_df[col] = token_df[col].apply(parse_string_number)
                # –í API –º—ã –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                token_df[f'{col}_capped'] = token_df[col] 

        if 'token_age' in token_df.columns:
            logger.debug("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–æ–ª–±—Ü–∞ 'token_age'")
            token_df['token_age_minutes'] = token_df['token_age'].apply(parse_time_to_minutes)
            token_df['token_age_hours'] = token_df['token_age_minutes'] / 60
            token_df['token_age_days'] = token_df['token_age_minutes'] / 1440
            token_df['is_very_new'] = (token_df['token_age_minutes'] < 60).astype(int)
            token_df['is_new'] = (token_df['token_age_minutes'] < 1440).astype(int)
            token_df['is_mature'] = (token_df['token_age_minutes'] > 10080).astype(int)
            token_df['is_very_mature'] = (token_df['token_age_minutes'] > 43200).astype(int)  # > 30 –¥–Ω–µ–π
            token_df['token_age_log'] = np.log1p(token_df['token_age_minutes'])
            token_df['token_age_sqrt'] = np.sqrt(token_df['token_age_minutes'])

        if 'top_10_holdings' in token_df.columns:
             logger.debug("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–æ–ª–±—Ü–∞ 'top_10_holdings'")
             # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è parse_top10_holdings
             top10_total = token_data.get('top_10_percent', None) 
             holdings_str = token_data.get('top_10_holdings', '') 
             metrics = parse_top10_holdings(holdings_str, top10_total)
             token_df['biggest_whale_percent'] = metrics['top1_real_percent']
             token_df['top3_whales_percent'] = metrics['top3_real_percent']
             token_df['top5_whales_percent'] = metrics['top5_real_percent']
             token_df['whale_dominance_index'] = metrics['concentration_ratio']
             token_df['gini_coefficient'] = metrics['gini_coefficient']
             token_df['herfindahl_index'] = metrics['herfindahl_index']
             for i in range(10):
                 token_df[f'whale_{i+1}_internal_share'] = metrics['internal_distribution'][i]

        # --- –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫–∞–∫ –≤ –Ω–æ—É—Ç–±—É–∫–µ) ---
        logger.debug("–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        # –¢–æ—Ä–≥–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        token_df['buy_sell_ratio_1m'] = np.where(token_df['sell_volume_1m'] > 0, 
                                               token_df['buy_volume_1m'] / token_df['sell_volume_1m'], 0)
        token_df['buy_sell_ratio_5m'] = np.where(token_df['sell_volume_5m'] > 0, 
                                               token_df['buy_volume_5m'] / token_df['sell_volume_5m'], 0)
        token_df['total_volume_1m'] = token_df['buy_volume_1m'] + token_df['sell_volume_1m']
        token_df['total_volume_5m'] = token_df['buy_volume_5m'] + token_df['sell_volume_5m']
        token_df['buy_pressure_1m'] = np.where(token_df['total_volume_1m'] > 0, 
                                             token_df['buy_volume_1m'] / token_df['total_volume_1m'], 0)
        token_df['buy_pressure_5m'] = np.where(token_df['total_volume_5m'] > 0, 
                                             token_df['buy_volume_5m'] / token_df['total_volume_5m'], 0)
        token_df['buy_pressure_change'] = token_df['buy_pressure_5m'] - token_df['buy_pressure_1m']
        token_df['avg_buy_size_1m'] = np.where(token_df['buys_1m'] > 0, 
                                   token_df['buy_volume_1m'] / token_df['buys_1m'], 0)
        token_df['avg_sell_size_1m'] = np.where(token_df['sells_1m'] > 0, 
                                    token_df['sell_volume_1m'] / token_df['sells_1m'], 0)
        token_df['avg_buy_size_5m'] = np.where(token_df['buys_5m'] > 0, 
                                   token_df['buy_volume_5m'] / token_df['buys_5m'], 0)
        token_df['avg_sell_size_5m'] = np.where(token_df['sells_5m'] > 0, 
                                    token_df['sell_volume_5m'] / token_df['sells_5m'], 0)
        token_df['buy_vs_sell_size_1m'] = np.where(token_df['avg_sell_size_1m'] > 0,
                                       token_df['avg_buy_size_1m'] / token_df['avg_sell_size_1m'], 0)
        token_df['buy_vs_sell_size_5m'] = np.where(token_df['avg_sell_size_5m'] > 0,
                                       token_df['avg_buy_size_5m'] / token_df['avg_sell_size_5m'], 0)
        token_df['volume_growth_1m_to_5m'] = np.where(token_df['volume_1m'] > 0,
                                          token_df['volume_5m'] / token_df['volume_1m'], 0)
        token_df['buy_growth_1m_to_5m'] = np.where(token_df['buys_1m'] > 0,
                                       token_df['buys_5m'] / token_df['buys_1m'], 0)
        token_df['sell_growth_1m_to_5m'] = np.where(token_df['sells_1m'] > 0,
                                        token_df['sells_5m'] / token_df['sells_1m'], 0)

        # –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏—Ö–æ–¥—è—Ç –≤ "–ø–ª–æ—Å–∫–æ–º" –≤–∏–¥–µ)
        logger.debug("–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        token_df['total_holders_emoji'] = (token_df.get('buyers_green', 0) + token_df.get('buyers_blue', 0) + 
                                         token_df.get('buyers_yellow', 0) + token_df.get('buyers_red', 0))
        token_df['total_snipers'] = (token_df.get('buyers_clown', 0) + token_df.get('buyers_sun', 0) + 
                                   token_df.get('buyers_moon_half', 0) + token_df.get('buyers_moon_new', 0))
        token_df['holders_keep_ratio'] = np.where(token_df['total_holders_emoji'] > 0,
                                      (token_df.get('buyers_green', 0) + token_df.get('buyers_blue', 0)) / token_df['total_holders_emoji'], 0)
        token_df['holders_sell_ratio'] = np.where(token_df['total_holders_emoji'] > 0,
                                      (token_df.get('buyers_yellow', 0) + token_df.get('buyers_red', 0)) / token_df['total_holders_emoji'], 0)
        token_df['holders_diamond_hands'] = np.where(token_df['total_holders_emoji'] > 0,
                                         token_df.get('buyers_green', 0) / token_df['total_holders_emoji'], 0)
        token_df['holders_paper_hands'] = np.where(token_df['total_holders_emoji'] > 0,
                                       token_df.get('buyers_red', 0) / token_df['total_holders_emoji'], 0)
        token_df['snipers_keep_ratio'] = np.where(token_df['total_snipers'] > 0,
                                      (token_df.get('buyers_clown', 0) + token_df.get('buyers_sun', 0)) / token_df['total_snipers'], 0)
        token_df['snipers_dump_ratio'] = np.where(token_df['total_snipers'] > 0,
                                      (token_df.get('buyers_moon_half', 0) + token_df.get('buyers_moon_new', 0)) / token_df['total_snipers'], 0)
        token_df['snipers_vs_holders_ratio'] = np.where(token_df['total_holders_emoji'] > 0,
                                            token_df['total_snipers'] / token_df['total_holders_emoji'], 0)
        token_df['total_active_addresses'] = token_df['total_holders_emoji'] + token_df['total_snipers']
        token_df['trust_score'] = (token_df.get('buyers_green', 0) + token_df.get('buyers_clown', 0)) / (token_df['total_active_addresses'] + 1)
        token_df['distrust_score'] = (token_df.get('buyers_red', 0) + token_df.get('buyers_moon_new', 0)) / (token_df['total_active_addresses'] + 1)

        # –†—ã–Ω–æ—á–Ω—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
        logger.debug("–°–æ–∑–¥–∞–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤...")
        token_df['liquidity_to_mcap_ratio'] = np.where(token_df.get('market_cap_capped', 0) > 0,
                                                     token_df.get('liquidity_capped', 0) / token_df['market_cap_capped'], 0)
        token_df['volume_to_liquidity_ratio'] = np.where(token_df.get('liquidity_capped', 0) > 0,
                                                       token_df['total_volume_5m'] / token_df['liquidity_capped'], 0)
        token_df['volume_to_mcap_ratio'] = np.where(token_df.get('market_cap_capped', 0) > 0,
                                        token_df['total_volume_5m'] / token_df['market_cap_capped'], 0)
        
        token_df['volume_per_sol'] = np.where(token_df.get('sol_pooled', 0) > 0,
                                      token_df['total_volume_5m'] / token_df['sol_pooled'], 0)
        token_df['mcap_per_sol'] = np.where(token_df.get('sol_pooled', 0) > 0,
                                    token_df.get('market_cap_capped', 0) / token_df['sol_pooled'], 0)
        
        token_df['ratio_change'] = token_df.get('current_ratio', 0) - token_df.get('initial_ratio', 0)
        token_df['ratio_change_percent'] = np.where(token_df.get('initial_ratio', 1) > 0,
                                        (token_df.get('current_ratio', 0) - token_df.get('initial_ratio', 0)) / token_df['initial_ratio'], 0)

        # –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è –∏ –¥–µ—Ä–∂–∞—Ç–µ–ª–∏
        logger.debug("–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏...")
        if 'total_holders' in token_df.columns:
            token_df['volume_per_holder'] = np.where(token_df['total_holders'] > 0,
                                         token_df['total_volume_5m'] / token_df['total_holders'], 0)
            token_df['mcap_per_holder'] = np.where(token_df['total_holders'] > 0,
                                       token_df.get('market_cap_capped', 0) / token_df['total_holders'], 0)
            token_df['active_to_total_holders_ratio'] = np.where(token_df['total_holders'] > 0,
                                                     token_df['total_active_addresses'] / token_df['total_holders'], 0)
        
        if 'freshies_1d_percent' in token_df.columns and 'freshies_7d_percent' in token_df.columns:
            token_df['freshies_growth'] = token_df['freshies_7d_percent'] - token_df['freshies_1d_percent']
            token_df['veteran_ratio'] = 100 - token_df['freshies_7d_percent']
        
        if 'dev_current_balance_percent' in token_df.columns:
            token_df['dev_risk_high'] = (token_df['dev_current_balance_percent'] > 10).astype(int)
            token_df['dev_risk_medium'] = ((token_df['dev_current_balance_percent'] > 5) & 
                               (token_df['dev_current_balance_percent'] <= 10)).astype(int)

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
        logger.debug("–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏...")
        security_features = ['security_no_mint', 'security_blacklist', 'security_burnt', 
                            'security_dev_sold', 'security_dex_paid']
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        for sf in security_features:
             if sf not in token_df.columns:
                  token_df[sf] = 0 # –∏–ª–∏ False –¥–ª—è –±—É–ª–µ–≤—ã—Ö
        
        token_df['security_score'] = token_df[security_features].sum(axis=1)
        token_df['security_perfect'] = (token_df['security_score'] == 5).astype(int)
        token_df['security_risky'] = (token_df['security_score'] <= 2).astype(int)

        # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        logger.debug("–°–æ–∑–¥–∞–Ω–∏–µ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π...")
        log_features = ['market_cap_capped', 'liquidity_capped', 'ath_capped', 'total_holders', 'total_volume_5m', 'sol_pooled']
        for col in log_features:
            if col in token_df.columns:
                token_df[f'{col}_log'] = np.log1p(token_df[col].fillna(0))
                token_df[f'{col}_sqrt'] = np.sqrt(token_df[col].fillna(0))
                token_df[f'{col}_inv'] = 1 / (token_df[col].fillna(0) + 1)

        # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        logger.debug("–°–æ–∑–¥–∞–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        token_df['volume_liquidity_interaction'] = token_df['total_volume_5m'] * token_df.get('liquidity_capped_log', 0)
        token_df['volume_mcap_interaction'] = token_df['total_volume_5m'] * token_df.get('market_cap_capped_log', 0)
        token_df['age_volume_interaction'] = token_df.get('token_age_log', 0) * token_df['total_volume_5m']
        token_df['age_holders_interaction'] = token_df.get('token_age_log', 0) * np.log1p(token_df.get('total_holders', 1))
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –Ω–æ—É—Ç–±—É–∫–∞, –µ—Å–ª–∏ –æ–Ω–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç, –ø–æ–¥—Å—Ç–∞–≤–ª—è–µ–º 0
        token_df['trust_whale_interaction'] = token_df.get('trust_score', 0) * token_df.get('whale_centralization', 0) 
        token_df['distrust_whale_interaction'] = token_df.get('distrust_score', 0) * token_df.get('dangerous_whale_concentration', 0) 

        # --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é ---
        logger.debug("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
        available_features = [col for col in features if col in token_df.columns]
        missing_features = [col for col in features if col not in token_df.columns]
        
        if missing_features:
            logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}. –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏.")
            for col in missing_features:
                token_df[col] = 0
        
        X_token = token_df[features].fillna(0)
        logger.debug(f"–§–æ—Ä–º–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {X_token.shape}")

        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫–∞–∫ –≤ –Ω–æ—É—Ç–±—É–∫–µ)
        logger.debug("–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        for col in X_token.columns:
            if col in encoders:
                try:
                    le = encoders[col]
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–µ—Ç–æ–∫
                    known_vals = set(le.classes_)
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º 'unknown' –µ—Å–ª–∏ –æ–Ω–æ –±—ã–ª–æ –≤ –æ–±—É—á–µ–Ω–∏–∏, –∏–Ω–∞—á–µ –ø–µ—Ä–≤—ã–π –∏–∑–≤–µ—Å—Ç–Ω—ã–π –∫–ª–∞—Å—Å
                    default_val = 'unknown' if 'unknown' in known_vals else (le.classes_[0] if len(le.classes_) > 0 else '')
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    X_token[col] = X_token[col].apply(lambda x: x if str(x) in known_vals else default_val)
                    X_token[col] = le.transform(X_token[col].astype(str))
                    logger.debug(f"–ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω –ø—Ä–∏–∑–Ω–∞–∫ '{col}'")
                except Exception as e:
                     logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è '{col}': {e}. –ó–∞–ø–æ–ª–Ω—è–µ–º 0.")
                     X_token[col] = 0 # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (–∫–∞–∫ –≤ –Ω–æ—É—Ç–±—É–∫–µ)
        logger.debug("–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
        X_token_scaled = scaler.transform(X_token)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–∫–∞–∫ –≤ –Ω–æ—É—Ç–±—É–∫–µ)
        logger.debug("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
        if model_name == 'Ensemble' and isinstance(model, dict) and ensemble_weights is not None:
            logger.debug("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–Ω—Å–∞–º–±–ª—å.")
            ensemble_probas = []
            model_names = list(model.keys()) 
            for name in model_names:
                m = model[name]
                try:
                    prob = m.predict_proba(X_token_scaled)[0, 1]
                    ensemble_probas.append(prob)
                    logger.debug(f"–ú–æ–¥–µ–ª—å {name} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {prob}")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ {name}: {e}")
                    ensemble_probas.append(0.5) 
            
            if ensemble_probas:
                 probability = np.average(ensemble_probas, weights=ensemble_weights)
                 confidence_interval = (np.min(ensemble_probas), np.max(ensemble_probas))
                 logger.debug(f"–ê–Ω—Å–∞–º–±–ª—å: —Å—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å={probability}, –∏–Ω—Ç–µ—Ä–≤–∞–ª={confidence_interval}")
            else:
                 probability = 0.5
                 confidence_interval = (0.0, 1.0)
        else:
            logger.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–¥–∏–Ω–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å: {model_name}.")
            try:
                probability = model.predict_proba(X_token_scaled)[0, 1]
                # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
                confidence_interval = (max(probability * 0.9, 0.0), min(probability * 1.1, 1.0))
                logger.debug(f"–û–¥–∏–Ω–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å={probability}")
            except Exception as e:
                 logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
                 probability = 0.5
                 confidence_interval = (0.0, 1.0)

        prediction = int(probability > 0.5)
        
        if probability >= 0.85:
            recommendation = "üî• VERY STRONG BUY - –ò—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∏–µ —à–∞–Ω—Å—ã –Ω–∞ —Ä–æ—Å—Ç!"
        elif probability >= 0.75:
            recommendation = "üöÄ STRONG BUY - –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–µ —à–∞–Ω—Å—ã –Ω–∞ —Ä–æ—Å—Ç"
        elif probability >= 0.65:
            recommendation = "‚úÖ BUY - –•–æ—Ä–æ—à–∏–µ —à–∞–Ω—Å—ã –Ω–∞ —Ä–æ—Å—Ç"
        elif probability >= 0.55:
            recommendation = "‚öñÔ∏è CONSIDER - –£–º–µ—Ä–µ–Ω–Ω—ã–µ —à–∞–Ω—Å—ã, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ"
        elif probability >= 0.45:
            recommendation = "‚ö†Ô∏è CAUTION - –ù–∏–∑–∫–∏–µ —à–∞–Ω—Å—ã, –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫"
        elif probability >= 0.35:
            recommendation = "‚ùå AVOID - –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–µ —à–∞–Ω—Å—ã –Ω–∞ —Ä–æ—Å—Ç"
        else:
            recommendation = "üö´ STRONG AVOID - –ö—Ä–∞–π–Ω–µ –Ω–∏–∑–∫–∏–µ —à–∞–Ω—Å—ã"

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–µ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        result = {
            "success": True,
            "probability": float(probability),
            "prediction": prediction, 
            "recommendation": recommendation,
            "confidence_interval": [float(confidence_interval[0]), float(confidence_interval[1])]
        }
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–º–≤–æ–ª –∏ –∏–º—è —Ç–æ–∫–µ–Ω–∞, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if 'symbol' in token_data:
            result['token_symbol'] = token_data['symbol']
        if 'name' in token_data:
            result['token_name'] = token_data['name']

        return result

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ predict_memtoken_advanced: {str(e)}", exc_info=True)
        return {
            "success": False,
            "error": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}",
            "probability": 0.5,
            "prediction": 0,
            "recommendation": "‚ùì –û–®–ò–ë–ö–ê - –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ",
            "confidence_interval": [0.0, 1.0]
        }

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∏–∑ —è—á–µ–π–∫–∏ 20 –Ω–æ—É—Ç–±—É–∫–∞) ---
def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ –µ—ë –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã."""
    global model, scaler, encoders, features, model_name, ensemble_weights
    try:
        # –ò–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏ (–¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –∏–º–µ–Ω–∞–º–∏ —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –∑–∞–≥—Ä—É–∂–∞–µ—Ç–µ –Ω–∞ Railway)
        model_path = 'memtoken_model_improved.pkl'
        scaler_path = 'memtoken_scaler_improved.pkl'
        encoders_path = 'memtoken_encoders_improved.pkl' 
        features_path = 'memtoken_features_improved.pkl'
        metadata_path = 'memtoken_model_metadata.json'
        ensemble_weights_path = 'memtoken_ensemble_weights.pkl'

        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ {model_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        if not os.path.exists(scaler_path):
            raise FileNotFoundError(f"–§–∞–π–ª —Å–∫–µ–π–ª–µ—Ä–∞ {scaler_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        if not os.path.exists(encoders_path):
            raise FileNotFoundError(f"–§–∞–π–ª —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ {encoders_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        if not os.path.exists(features_path):
            raise FileNotFoundError(f"–§–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ {features_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")

        logger.info(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–∞–π–¥–µ–Ω. –†–∞–∑–º–µ—Ä: {os.path.getsize(model_path)} –±–∞–π—Ç")
        logger.info(f"–§–∞–π–ª —Å–∫–µ–π–ª–µ—Ä–∞ –Ω–∞–π–¥–µ–Ω. –†–∞–∑–º–µ—Ä: {os.path.getsize(scaler_path)} –±–∞–π—Ç")
        logger.info(f"–§–∞–π–ª —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –Ω–∞–π–¥–µ–Ω. –†–∞–∑–º–µ—Ä: {os.path.getsize(encoders_path)} –±–∞–π—Ç")
        logger.info(f"–§–∞–π–ª –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞–π–¥–µ–Ω. –†–∞–∑–º–µ—Ä: {os.path.getsize(features_path)} –±–∞–π—Ç")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        model = joblib.load(model_path)
        logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
        
        scaler = joblib.load(scaler_path)
        logger.info("–°–∫–µ–π–ª–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω.")
        
        encoders = joblib.load(encoders_path) 
        logger.info("–≠–Ω–∫–æ–¥–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
        
        features = joblib.load(features_path) 
        logger.info(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {len(features) if features else 'N/A'}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        if os.path.exists(metadata_path):
             with open(metadata_path, 'r') as f:
                 metadata = json.load(f)
             model_name = metadata.get('best_model_name', 'Unknown')
             logger.info(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –¢–∏–ø –º–æ–¥–µ–ª–∏: {model_name}")
             if model_name == 'Ensemble' and os.path.exists(ensemble_weights_path):
                  ensemble_weights = np.array(joblib.load(ensemble_weights_path))
                  logger.info("–í–µ—Å–∞ –∞–Ω—Å–∞–º–±–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
             elif model_name == 'Ensemble':
                  logger.warning(f"–ú–æ–¥–µ–ª—å - –∞–Ω—Å–∞–º–±–ª—å, –Ω–æ —Ñ–∞–π–ª –≤–µ—Å–æ–≤ {ensemble_weights_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        else:
             # –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, –µ—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å, —Ç–æ –∞–Ω—Å–∞–º–±–ª—å
             model_name = 'Ensemble' if isinstance(model, dict) else 'Single_Model'
             logger.info(f"–ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: {model_name}")

        logger.info("‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}", exc_info=True)
        # –ù–∞ Railway —ç—Ç–æ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ —Ç–æ–º—É, —á—Ç–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Å–º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã,
        # –Ω–æ –ª–æ–≥–∏ –±—É–¥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ.

# --- –ú–∞—Ä—à—Ä—É—Ç—ã Flask ---

@app.route('/')
def home():
    return jsonify({
        "message": "API –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —É—Å–ø–µ—Ö–∞ –º–µ–º—Ç–æ–∫–µ–Ω–æ–≤",
        "status": "running",
        "timestamp": datetime.utcnow().isoformat() + "Z"
    })

@app.route('/predict', methods=['POST'])
def predict():
    """Endpoint –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è."""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—ã—Ä—ã–µ JSON –¥–∞–Ω–Ω—ã–µ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        raw_data = request.get_json()
        logger.debug(f"–ü–æ–ª—É—á–µ–Ω —Å—ã—Ä—ã–µ JSON: {raw_data}")

        # --- –ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤—Ö–æ–¥—è—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å "body" ---
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ "body", –µ—Å–ª–∏ –æ–Ω–∏ —Ç–∞–º
        if isinstance(raw_data, dict) and 'body' in raw_data:
            try:
                # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ 'body' - —ç—Ç–æ JSON —Å—Ç—Ä–æ–∫–∞
                data = json.loads(raw_data['body'])
                logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –∏–∑ 'body': {data}")
            except json.JSONDecodeError as e:
                logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –∏–∑ 'body': {e}")
                return jsonify({"error": "–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON –≤ –ø–æ–ª–µ 'body'"}), 400
        else:
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏—à–ª–∏ –Ω–∞–ø—Ä—è–º—É—é
            data = raw_data
        # --- –ö–æ–Ω–µ—Ü –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ ---

        if not data:
            return jsonify({"error": "JSON –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã"}), 400

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Å–ø–∏—Å–æ–∫ –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å)
        if isinstance(data, list) and len(data) > 0:
             token_data = data[0] # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –∏–∑ —Å–ø–∏—Å–∫–∞
        elif isinstance(data, dict):
             token_data = data
        else:
             return jsonify({"error": "–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö. –û–∂–∏–¥–∞–µ—Ç—Å—è JSON –æ–±—ä–µ–∫—Ç –∏–ª–∏ –º–∞—Å—Å–∏–≤ —Å –æ–±—ä–µ–∫—Ç–æ–º."}), 400

        # --- –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ –æ–∂–∏–¥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ—É–Ω–∫—Ü–∏–∏ ---
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –≤ –ø–ª–æ—Å–∫–∏–µ –ø–æ–ª—è
        formatted_data = {}

        # –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è
        for key, value in token_data.items():
            if key not in ['first_buyers', 'current_initial_ratio', 'security']:
                formatted_data[key] = value

        # first_buyers
        if 'first_buyers' in token_data and isinstance(token_data['first_buyers'], dict):
            fb = token_data['first_buyers']
            formatted_data['buyers_green'] = fb.get('green', 0)
            formatted_data['buyers_blue'] = fb.get('blue', 0)
            formatted_data['buyers_yellow'] = fb.get('yellow', 0)
            formatted_data['buyers_red'] = fb.get('red', 0)
            formatted_data['buyers_clown'] = fb.get('clown', 0)
            formatted_data['buyers_sun'] = fb.get('sun', 0)
            formatted_data['buyers_moon_half'] = fb.get('moon_half', 0)
            formatted_data['buyers_moon_new'] = fb.get('moon_new', 0)

        # current_initial_ratio
        if 'current_initial_ratio' in token_data and isinstance(token_data['current_initial_ratio'], dict):
            cir = token_data['current_initial_ratio']
            formatted_data['current_ratio'] = cir.get('current', 0)
            formatted_data['initial_ratio'] = cir.get('initial', 0)

        # security
        if 'security' in token_data and isinstance(token_data['security'], dict):
            sec = token_data['security']
            formatted_data['security_no_mint'] = int(sec.get('no_mint', False))
            formatted_data['security_blacklist'] = int(sec.get('blacklist', False))
            formatted_data['security_burnt'] = int(sec.get('burnt', False))
            formatted_data['security_dev_sold'] = int(sec.get('dev_sold', False))
            formatted_data['security_dex_paid'] = int(sec.get('dex_paid', False))

        # --- –í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ---
        result = predict_memtoken_advanced(formatted_data)

        return jsonify(result)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ /predict: {str(e)}", exc_info=True)
        return jsonify({
            "success": False,
            "error": f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {str(e)}"
        }), 500

# --- –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ---
if __name__ == '__main__':
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    load_model()
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ä—Ç –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è (Railway) –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º 5000 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    port = int(os.environ.get('PORT', 5000))
    
    # –ó–∞–ø—É—Å–∫ Flask –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (–Ω–∞ Railway –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è Gunicorn)
    # host='0.0.0.0' –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è Railway
    app.run(host='0.0.0.0', port=port, debug=False) 
